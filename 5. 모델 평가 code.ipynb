{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993d5d0",
   "metadata": {},
   "source": [
    "# 1. Fine-tuning 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fb91d",
   "metadata": {},
   "source": [
    "## 1-1. LLM기반 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096659ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 10:07:46 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('test_dataset')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"eddyfox8812/llama-3-8b-otc-rag-ko-checkpotint-594\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst=[]\n",
    "questions=[]\n",
    "contexts=[]\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    question = text.split('<|start_header_id|>user<|end_header_id|>')[1].split('<|eot_id|>')[0].strip()\n",
    "    context = text.split('검색 결과:\\n-----')[1].split('<|eot_id|>')[0].strip()\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)\n",
    "    questions.append(question)\n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4aaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'진코넥정240밀리그램은 언제 복용해야 하나요?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e672fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n검색 결과에는 진코넥정240밀리그램의 복용 시기를 찾을 수 없습니다.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_lst[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params= SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62489f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 197/197 [00:24<00:00,  8.16it/s, est. speed input: 12787.42 toks/s, output: 532.98 toks/s]\n"
     ]
    }
   ],
   "source": [
    "preds = vllm_model.generate(prompt_lst, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121daf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [pred.outputs[0].text for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0675f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = \"api key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22686341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_with_llm(questions, contexts, predictions, labels):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "당신은 RAG(Retrieval-Augmented Generation) 시스템 평가 전문가입니다. 아래 정보를 바탕으로 생성된 답변의 품질을 철저히 평가해주세요.\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "검색된 컨텍스트:\n",
    "{context}\n",
    "\n",
    "생성된 답변:\n",
    "{prediction}\n",
    "\n",
    "참조 답변(정답):\n",
    "{label}\n",
    "\n",
    "다음 4가지 평가 기준으로 1-5점 척도로 점수를 매겨주세요:\n",
    "\n",
    "1. 응답 정확성 (Answer Correctness) [1-5]:\n",
    "   * 생성된 답변이 참조 답변과 비교하여 정확하고 완전한 정보를 제공하는지 평가\n",
    "   * 1점: 완전히 잘못된 정보\n",
    "   * 2점: 부분적으로 관련된 정보를 담고 있으나 대부분 부정확함\n",
    "   * 3점: 정확한 정보와 부정확한 정보가 혼재되어 있음\n",
    "   * 4점: 대부분 정확하지만 일부 정보가 누락되거나 미미한 오류가 있음\n",
    "   * 5점: 참조 답변과 비교했을 때 완전히 정확하고 포괄적인 정보를 제공함\n",
    "\n",
    "2. 컨텍스트 관련성 (Context Relevance) [1-5]:\n",
    "   * 검색된 컨텍스트가 질문에 대답하기 위해 관련성이 높은지 평가\n",
    "   * 1점: 컨텍스트가 질문과 전혀 관련이 없음\n",
    "   * 2점: 컨텍스트가 질문과 간접적으로만 관련됨\n",
    "   * 3점: 컨텍스트 중 일부만 질문과 직접적으로 관련됨\n",
    "   * 4점: 대부분의 컨텍스트가 질문과 직접적으로 관련됨\n",
    "   * 5점: 모든 컨텍스트가 질문에 완벽하게 관련되어 있고 불필요한 정보가 없음\n",
    "\n",
    "3. 컨텍스트 충실성 (Context Faithfulness) [1-5]:\n",
    "   * 생성된 답변이 주어진 컨텍스트에만 기반하는지, 아니면 없는 정보를 추가했는지 평가\n",
    "   * 1점: 답변이 컨텍스트에 없는 정보로만 구성됨 (심각한 환각)\n",
    "   * 2점: 답변이 주로 컨텍스트에 없는 정보로 구성됨\n",
    "   * 3점: 답변이 컨텍스트 정보와 없는 정보가 혼합되어 있음\n",
    "   * 4점: 답변이 주로 컨텍스트에 기반하지만 약간의 추가 정보가 있음\n",
    "   * 5점: 답변이 전적으로 컨텍스트에 있는 정보만을 사용함\n",
    "\n",
    "4. 컨텍스트 충분성 (Context Recall) [1-5]:\n",
    "   * 검색된 컨텍스트가 질문에 완전히 답변하기에 충분한 정보를 포함하는지 평가\n",
    "   * 1점: 컨텍스트가 답변에 필요한 정보를 전혀 포함하지 않음\n",
    "   * 2점: 컨텍스트가 필요한 정보의 일부만 포함함\n",
    "   * 3점: 컨텍스트가 필요한 정보의 약 절반을 포함함\n",
    "   * 4점: 컨텍스트가 필요한 정보의 대부분을 포함하지만 일부 누락됨\n",
    "   * 5점: 컨텍스트가 질문에 완전히 답변하기 위한 모든 필요한 정보를 포함함\n",
    "\n",
    "반드시 다음 JSON 형식으로만 응답하세요. 마크다운은 사용하지 않습니다.:\n",
    "{\n",
    "  \"answer_correctness\": 정수로 된 점수(1-5),\n",
    "  \"context_relevance\": 정수로 된 점수(1-5),\n",
    "  \"context_faithfulness\": 정수로 된 점수(1-5),\n",
    "  \"context_recall\": 점수(1-5),\n",
    "  \"analysis\": \"종합적인 분석 의견\"\n",
    "}\n",
    "\n",
    "다른 형식의 응답은 하지 마세요. 오직 마크다운이 아닌 JSON만 반환하세요.\n",
    "\"\"\"\n",
    "    \n",
    "    for i in tqdm(range(len(questions)), total=len(questions), desc=\"RAG 평가 진행 중\"):\n",
    "        try:\n",
    "            \n",
    "            prompt = prompt_template\n",
    "            prompt = prompt.replace(\"{question}\", str(questions[i]) if questions[i] is not None else \"\")\n",
    "            prompt = prompt.replace(\"{context}\", str(contexts[i]) if contexts[i] is not None else \"\")\n",
    "            prompt = prompt.replace(\"{prediction}\", str(predictions[i]) if predictions[i] is not None else \"\")\n",
    "            prompt = prompt.replace(\"{label}\", str(labels[i]) if labels[i] is not None else \"\")\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"당신은 RAG 평가 도구입니다. 반드시 유효한 JSON 형식으로만 응답하세요.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            answer_correctness = result['answer_correctness']\n",
    "            context_relevance = result['context_relevance']\n",
    "            context_faithfulness = result['context_faithfulness']\n",
    "            context_recall = result['context_recall']\n",
    "            \n",
    "            total_score = answer_correctness + context_relevance + context_faithfulness + context_recall\n",
    "            \n",
    "            row_result = {\n",
    "                'id': i,\n",
    "                'question': questions[i],\n",
    "                'answer_correctness': answer_correctness,\n",
    "                'context_relevance': context_relevance,\n",
    "                'context_faithfulness': context_faithfulness,\n",
    "                'context_recall': context_recall,\n",
    "                'total_score': total_score,\n",
    "                'analysis': result['analysis']\n",
    "            }\n",
    "            \n",
    "            results.append(row_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"항목 {i} 평가 중 오류 발생: {e}\")\n",
    "            results.append({\n",
    "                'id': i,\n",
    "                'question': questions[i],\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if 'total_score' in results_df.columns:\n",
    "        metrics_summary = {\n",
    "            '평균 총점': results_df['total_score'].mean(),\n",
    "            '응답 정확성 평균': results_df['answer_correctness'].mean(),\n",
    "            '컨텍스트 관련성 평균': results_df['context_relevance'].mean(),\n",
    "            '컨텍스트 충실성 평균': results_df['context_faithfulness'].mean(),\n",
    "            '컨텍스트 충분성 평균': results_df['context_recall'].mean()\n",
    "        }\n",
    "        print(\"\\n===== 평가 요약 =====\")\n",
    "        for metric, value in metrics_summary.items():\n",
    "            print(f\"{metric}: {value:.2f}\")\n",
    "    \n",
    "    return results_df, metrics_summary if 'total_score' in results_df.columns else results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2640d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG 평가 진행 중: 100%|██████████| 197/197 [09:59<00:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 평가 요약 =====\n",
      "평균 총점: 18.71\n",
      "응답 정확성 평균: 4.85\n",
      "컨텍스트 관련성 평균: 4.71\n",
      "컨텍스트 충실성 평균: 4.99\n",
      "컨텍스트 충분성 평균: 4.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, metrics_summary = evaluate_rag_with_llm(questions, contexts, preds, label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64666a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_relevance</th>\n",
       "      <th>context_faithfulness</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>total_score</th>\n",
       "      <th>analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>덴플러스포르테캡슐의 성분에 대해 설명해 줄래?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>생성된 답변은 참조 답변과 동일하게 덴플러스포르테캡슐의 성분에 대한 설명을 찾을 수...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>케어스킨로션을 사용하면 생리통이 완화될까요?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>생성된 답변은 참조 답변과 동일하게 검색된 컨텍스트에서 케어스킨로션이 생리통 완화에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>테라미플러스크림을 어느 약국에서 구할 수 있는지 아는 사람 있어?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>생성된 답변은 참조 답변과 완전히 일치하며, 질문에 대한 정확하고 포괄적인 정보를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>클리어드롭점안액의 유통기한은 얼마인가요?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>생성된 답변은 참조 답변과 일치하며, 클리어드롭점안액의 유통기한에 대한 정보가 없다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>모이스뷰점안액의 사용 금지는 어떤 제품으로부터 영향을 받나요?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있으며, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                              question  answer_correctness  \\\n",
       "0   0             덴플러스포르테캡슐의 성분에 대해 설명해 줄래?                   5   \n",
       "1   1              케어스킨로션을 사용하면 생리통이 완화될까요?                   5   \n",
       "2   2  테라미플러스크림을 어느 약국에서 구할 수 있는지 아는 사람 있어?                   5   \n",
       "3   3                클리어드롭점안액의 유통기한은 얼마인가요?                   5   \n",
       "4   4    모이스뷰점안액의 사용 금지는 어떤 제품으로부터 영향을 받나요?                   5   \n",
       "\n",
       "   context_relevance  context_faithfulness  context_recall  total_score  \\\n",
       "0                  5                     5               2           17   \n",
       "1                  5                     5               5           20   \n",
       "2                  5                     5               5           20   \n",
       "3                  5                     5               2           17   \n",
       "4                  5                     5               2           17   \n",
       "\n",
       "                                            analysis  \n",
       "0  생성된 답변은 참조 답변과 동일하게 덴플러스포르테캡슐의 성분에 대한 설명을 찾을 수...  \n",
       "1  생성된 답변은 참조 답변과 동일하게 검색된 컨텍스트에서 케어스킨로션이 생리통 완화에...  \n",
       "2  생성된 답변은 참조 답변과 완전히 일치하며, 질문에 대한 정확하고 포괄적인 정보를 ...  \n",
       "3  생성된 답변은 참조 답변과 일치하며, 클리어드롭점안액의 유통기한에 대한 정보가 없다...  \n",
       "4  생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있으며, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'평균 총점': 18.705583756345177, '응답 정확성 평균': 4.852791878172589, '컨텍스트 관련성 평균': 4.7055837563451774, '컨텍스트 충실성 평균': 4.99492385786802, '컨텍스트 충분성 평균': 4.152284263959391}\n"
     ]
    }
   ],
   "source": [
    "print(metrics_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aefcb8",
   "metadata": {},
   "source": [
    "## 1-2. 인용문서 기반 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ref_numbers(text: str)  -> List[int]:\n",
    "    pattern = r'\\[\\[ref(\\d+)\\]\\]'\n",
    "\n",
    "    numbers = [int(match.group(1)) for match in re.finditer(pattern, text)]\n",
    "\n",
    "    return sorted(list(set(numbers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf889a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(labels, predictions) : \n",
    "\n",
    "    is_nested_list = isinstance(labels, list) and len(labels) > 0 and isinstance(labels[0], list)\n",
    "\n",
    "    if not is_nested_list : \n",
    "        labels = [labels]\n",
    "        predictions = [predictions]\n",
    "\n",
    "    if len(labels) != len(predictions) : \n",
    "        raise ValueError(\"라벨과 예측의 길이가 일치하지 않습니다\")\n",
    "\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "\n",
    "    for sample_labels, sample_preds in zip(labels, predictions) : \n",
    "        if not sample_labels : \n",
    "            sample_labels = set()\n",
    "        else : \n",
    "            sample_labels = set(sample_labels)\n",
    "\n",
    "        if not sample_preds : \n",
    "            sample_preds = set()\n",
    "        else : \n",
    "            sample_preds = set(sample_preds)\n",
    "\n",
    "        true_positives = len(sample_labels.intersection(sample_preds))\n",
    "        false_positives = len(sample_preds - sample_labels)\n",
    "        false_negatives = len(sample_labels - sample_preds)\n",
    "\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "\n",
    "    if sum(bool(l) for l in labels) == 0 and sum(bool(p) for p in predictions) == 0 : \n",
    "        return 1.0\n",
    "\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d85a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ref_numbers = []\n",
    "pred_ref_numbers = []\n",
    "\n",
    "for label, pred in zip(label_lst, preds) : \n",
    "    label_ref_numbers.append(extract_ref_numbers(label))\n",
    "    pred_ref_numbers.append(extract_ref_numbers(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b9e7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5], [1], [3], [5], [1], [4], [4], [2], [1], [4]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_ref_numbers[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d8186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5], [1], [3], [5], [1], [4], [4], [2], [1], [4]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_ref_numbers[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = calculate_f1_score(label_ref_numbers, pred_ref_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d73e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9847328244274809\n"
     ]
    }
   ],
   "source": [
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f302e2",
   "metadata": {},
   "source": [
    "# 2. Base Model 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb4377",
   "metadata": {},
   "source": [
    "## 3-1. LLM 기반 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ebb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d41a1c392e489d9b03fb684b12632c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 11:01:24 [config.py:585] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "INFO 02-12 11:01:24 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451fd683d6f440ecaa0a2bd90ed54621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590eb26e4666429f8b3b8f5af4d5c166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a136a4c7f76d4ca693566769df1e9fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 11:01:27 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='NCSOFT/Llama-VARCO-8B-Instruct', speculative_config=None, tokenizer='NCSOFT/Llama-VARCO-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=NCSOFT/Llama-VARCO-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 02-12 11:01:27 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7d577ff4bfd0>\n",
      "INFO 02-12 11:01:28 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 02-12 11:01:28 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 02-12 11:01:28 [gpu_model_runner.py:1174] Starting to load model NCSOFT/Llama-VARCO-8B-Instruct...\n",
      "WARNING 02-12 11:01:28 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 02-12 11:01:29 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d1cac1fedc4039bc01329d869a2813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66b6b2a7a8d4d7bb3b9658b312600fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3a4fc15356411596f9daffe2b505b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c95c673f904298b7b7113a01525baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 11:01:36 [weight_utils.py:281] Time spent downloading weights for NCSOFT/Llama-VARCO-8B-Instruct: 7.800788 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05dd32f03f24056b1d260c92dc060e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0144e642d9ef4943a62e61ac023d1786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 11:01:39 [loader.py:447] Loading weights took 2.40 seconds\n",
      "INFO 02-12 11:01:39 [gpu_model_runner.py:1186] Model loading took 14.9596 GB and 10.968036 seconds\n",
      "INFO 02-12 11:01:48 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/cf07419ff9/rank_0_0 for vLLM's torch.compile\n",
      "INFO 02-12 11:01:48 [backends.py:425] Dynamo bytecode transform time: 8.45 s\n",
      "INFO 02-12 11:01:51 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 02-12 11:02:15 [backends.py:144] Compiling a graph for general shape takes 26.59 s\n",
      "INFO 02-12 11:02:28 [monitor.py:33] torch.compile takes 35.04 s in total\n",
      "INFO 02-12 11:02:29 [kv_cache_utils.py:566] GPU KV cache size: 417,952 tokens\n",
      "INFO 02-12 11:02:29 [kv_cache_utils.py:569] Maximum concurrency for 8,192 tokens per request: 51.02x\n",
      "INFO 02-12 11:02:55 [gpu_model_runner.py:1534] Graph capturing finished in 27 secs, took 0.52 GiB\n",
      "INFO 02-12 11:02:56 [core.py:151] init engine (profile, create kv cache, warmup model) took 76.23 seconds\n"
     ]
    }
   ],
   "source": [
    "vllm_model = LLM(\n",
    "    model=\"NCSOFT/Llama-VARCO-8B-Instruct\",\n",
    "    dtype=\"bfloat16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1a2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fea9d8726f48949352b85890412d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dad81eb1ba84343b881b09843227a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7357bfcd6e14accb2e4db7b776dfedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = load_from_disk('test_dataset')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"eddyfox8812/llama-3-8b-otc-rag-ko-checkpotint-594\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = [] # LLM의 입력\n",
    "label_lst = [] # LLM의 답변\n",
    "questions = [] # 사용자의 질문\n",
    "contexts = [] # 검색 결과\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    question = text.split('<|start_header_id|>user<|end_header_id|>')[1].split('<|eot_id|>')[0].strip()\n",
    "    context = text.split('검색 결과:\\n-----')[1].split('<|eot_id|>')[0].strip()\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)\n",
    "    questions.append(question)\n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d3bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'건아이점안액 사용 후 증상이 72시간 이상 지속되면 어떻게 해야 하나요?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad543ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n건아이점안액 사용 후 증상이 72시간 이상 지속되면 사용을 중지하고 의사와 상의해야 합니다. [[ref1]]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_lst[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2014b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 파라미터 설정\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01470fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 197/197 [00:39<00:00,  4.99it/s, est. speed input: 7821.18 toks/s, output: 1346.81 toks/s]\n"
     ]
    }
   ],
   "source": [
    "preds = vllm_model.generate(prompt_lst, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c000f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [pred.outputs[0].text for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e517f7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG 평가 진행 중: 100%|██████████| 197/197 [10:04<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 평가 요약 =====\n",
      "평균 총점: 17.94\n",
      "응답 정확성 평균: 4.57\n",
      "컨텍스트 관련성 평균: 4.57\n",
      "컨텍스트 충실성 평균: 4.78\n",
      "컨텍스트 충분성 평균: 4.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, metrics_summary = evaluate_rag_with_llm(questions, contexts, preds, label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fdaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_relevance</th>\n",
       "      <th>context_faithfulness</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>total_score</th>\n",
       "      <th>analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>덴플러스포르테캡슐의 성분에 대해 설명해 줄래?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>생성된 답변은 덴플러스포르테캡슐의 성분에 대한 정보가 없음을 정확하게 전달하고 있으...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>케어스킨로션을 사용하면 생리통이 완화될까요?</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>생성된 답변은 케어스킨로션이 생리통 완화에 효과적이라는 정보가 없음을 명확히 전달하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>테라미플러스크림을 어느 약국에서 구할 수 있는지 아는 사람 있어?</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>생성된 답변은 테라미플러스크림에 대한 구체적인 정보를 제공하지 못했지만, 질문에 대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>클리어드롭점안액의 유통기한은 얼마인가요?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>생성된 답변은 클리어드롭점안액의 유통기한에 대한 정보가 없음을 정확하게 전달하고 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>모이스뷰점안액의 사용 금지는 어떤 제품으로부터 영향을 받나요?</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>생성된 답변은 모이스뷰점안액의 사용 금지와 관련된 정보가 포함되어 있으나, 참조 답...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                              question  answer_correctness  \\\n",
       "0   0             덴플러스포르테캡슐의 성분에 대해 설명해 줄래?                   5   \n",
       "1   1              케어스킨로션을 사용하면 생리통이 완화될까요?                   5   \n",
       "2   2  테라미플러스크림을 어느 약국에서 구할 수 있는지 아는 사람 있어?                   4   \n",
       "3   3                클리어드롭점안액의 유통기한은 얼마인가요?                   5   \n",
       "4   4    모이스뷰점안액의 사용 금지는 어떤 제품으로부터 영향을 받나요?                   2   \n",
       "\n",
       "   context_relevance  context_faithfulness  context_recall  total_score  \\\n",
       "0                  5                     5               2           17   \n",
       "1                  4                     5               3           17   \n",
       "2                  5                     5               2           16   \n",
       "3                  5                     5               2           17   \n",
       "4                  3                     3               2           10   \n",
       "\n",
       "                                            analysis  \n",
       "0  생성된 답변은 덴플러스포르테캡슐의 성분에 대한 정보가 없음을 정확하게 전달하고 있으...  \n",
       "1  생성된 답변은 케어스킨로션이 생리통 완화에 효과적이라는 정보가 없음을 명확히 전달하...  \n",
       "2  생성된 답변은 테라미플러스크림에 대한 구체적인 정보를 제공하지 못했지만, 질문에 대...  \n",
       "3  생성된 답변은 클리어드롭점안액의 유통기한에 대한 정보가 없음을 정확하게 전달하고 있...  \n",
       "4  생성된 답변은 모이스뷰점안액의 사용 금지와 관련된 정보가 포함되어 있으나, 참조 답...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfa412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'평균 총점': 17.944162436548222, '응답 정확성 평균': 4.573604060913706, '컨텍스트 관련성 평균': 4.573604060913706, '컨텍스트 충실성 평균': 4.776649746192893, '컨텍스트 충분성 평균': 4.020304568527918}\n"
     ]
    }
   ],
   "source": [
    "print(metrics_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d13690",
   "metadata": {},
   "source": [
    "## 2-2. 인용문서 기반 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ref_numbers = []\n",
    "pred_ref_numbers = []\n",
    "\n",
    "for label, pred in zip(label_lst, preds):\n",
    "    label_ref_numbers.append(extract_ref_numbers(label))\n",
    "    pred_ref_numbers.append(extract_ref_numbers(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d55ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5], [1], [3], [5], [1], [4], [4], [2], [1], [4]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_ref_numbers[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64be8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [3], [], [], [], [4], [], [], []]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_ref_numbers[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e05ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = calculate_f1_score(label_ref_numbers, pred_ref_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08510638297872342\n"
     ]
    }
   ],
   "source": [
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
