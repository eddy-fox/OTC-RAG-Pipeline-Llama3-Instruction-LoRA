{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbf1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8047c19713f1452db30b07026be95ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 04:42:34 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import requests\n",
    "import shutil\n",
    "import re\n",
    "import gradio as gr\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from huggingface_hub import snapshot_download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7afd668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1289/983038980.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab404ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15f57fe2c2d464c9ac45fce8b1fa49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied to: /workspace/chroma_otc\n",
      "files: ['4b8e600a-a733-48d2-b496-0752def62f36', 'chroma.sqlite3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1289/3015298251.py:22: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 64015\n"
     ]
    }
   ],
   "source": [
    "REPO_ID = \"eddyfox8812/otc-chroma-db\" \n",
    "target_dir = \"/workspace/chroma_otc\"\n",
    "\n",
    "local_repo = snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\")\n",
    "\n",
    "src = os.path.join(local_repo, \"chroma_otc\")\n",
    "os.makedirs(\"/workspace\", exist_ok=True)\n",
    "if os.path.exists(target_dir):\n",
    "    shutil.rmtree(target_dir)\n",
    "shutil.copytree(src, target_dir)\n",
    "\n",
    "print(\"copied to:\", target_dir)\n",
    "print(\"files:\", os.listdir(target_dir))\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=target_dir,\n",
    "    collection_name=\"otc_chunks\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "print(\"count:\", vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556070b",
   "metadata": {},
   "source": [
    "- drug_names 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187312da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique drug_names: 8649\n",
      "sample: ['1. 포비딘인후스프레이액(포비돈요오드)(바닐라향), 2. 포비딘인후스프레이액(포비돈요오드)(청포도향)', '1.셀레나제100마이크로그램퍼오랄액(아셀렌산나트륨오수화물)2.셀레나제티퍼오랄액(아셀렌산나트륨오수화물)', '1.셀레뉴원오랄액(아셀렌산나트륨오수화물) 2.셀큐민185오랄액(아셀렌산나트륨오수화물)', '1.큐앤큐포비돈거즈볼-중(포비돈요오드)2.큐앤큐포비돈거즈볼-대(포비돈요오드)', '1.포리비돈인후스프레이(포비돈요오드)(바닐라향), 2.포리비돈인후스프레이(포비돈요오드)(청포도향)', '5-엠씨크림(리도카인)', '가그린목액', '가네리버연질캡슐175mg(밀크시슬열매건조엑스)', '가네리버연질캡슐350mg(밀크시슬열매건조엑스)', '가네맥스연질캡슐(밀크시슬열매건조엑스)']\n"
     ]
    }
   ],
   "source": [
    "col = vectorstore._collection\n",
    "n = col.count()\n",
    "\n",
    "drug_set = set()\n",
    "batch = 5000\n",
    "\n",
    "for offset in range(0, n, batch):\n",
    "    out = col.get(include=[\"metadatas\"], limit=batch, offset=offset)\n",
    "    for m in out[\"metadatas\"]:\n",
    "        if m and m.get(\"drug_name\"):\n",
    "            drug_set.add(m[\"drug_name\"])\n",
    "\n",
    "drug_names = sorted(drug_set)\n",
    "print(\"unique drug_names:\", len(drug_names))\n",
    "print(\"sample:\", drug_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_drugs(token: str, drug_names: list[str], top_n: int = 10):\n",
    "    token = token.strip()\n",
    "    cands = [d for d in drug_names if token in d]\n",
    "    cands = sorted(cands, key=len)\n",
    "    return cands[:top_n]\n",
    "\n",
    "def choose_best_token(user_query: str, drug_names: list[str], top_n: int = 10):\n",
    "    tokens = re.findall(r\"[가-힣A-Za-z0-9]+\", user_query)\n",
    "    tokens = [t for t in tokens if len(t) >= 2]\n",
    "\n",
    "    best_token, best_cands = None, []\n",
    "    for t in tokens:\n",
    "        cands = candidate_drugs(t, drug_names, top_n=top_n)\n",
    "        if len(cands) > len(best_cands):\n",
    "            best_token, best_cands = t, cands\n",
    "\n",
    "    return best_token, best_cands\n",
    "\n",
    "def pick_drug_name(user_query: str, top_n=10):\n",
    "    best_token, cands, tokens = choose_best_token(user_query, top_n=top_n)\n",
    "\n",
    "    print(\"\\n[token 후보]:\", tokens[:15])\n",
    "    print(\"[선택된 token]:\", best_token, \"| 후보 수:\", len(cands))\n",
    "\n",
    "    if not cands:\n",
    "        print(\"[drug_name 후보 없음] -> drug_name 필터 없이 진행\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n[drug_name 후보 목록]\")\n",
    "    for idx, name in enumerate(cands, 1):\n",
    "        print(f\"{idx}. {name}\")\n",
    "\n",
    "    while True:\n",
    "        sel = input(\"번호 선택(Enter=1): \").strip()\n",
    "        if sel == \"\":\n",
    "            sel_idx = 1\n",
    "            break\n",
    "        if sel.isdigit():\n",
    "            sel_idx = int(sel)\n",
    "            if 1 <= sel_idx <= len(cands):\n",
    "                break\n",
    "        print(f\"⚠️ 1~{len(cands)} 사이 숫자를 입력하세요.\")\n",
    "\n",
    "    chosen = cands[sel_idx - 1]\n",
    "    print(\"선택된 drug_name:\", chosen)\n",
    "    return chosen\n",
    "\n",
    "def search_from_user_query(user_query: str, k=6, top_n=10, interactive=True):\n",
    "    chosen_drug_name = pick_drug_name(user_query, drug_names, top_n=top_n, interactive=interactive)\n",
    "\n",
    "    if chosen_drug_name:\n",
    "        hits = vectorstore.similarity_search(user_query, k=k, filter={\"drug_name\": chosen_drug_name})\n",
    "        print(\"[필터검색] hits:\", len(hits))\n",
    "        return chosen_drug_name, hits\n",
    "\n",
    "    hits = vectorstore.similarity_search(user_query, k=k)\n",
    "    print(\"[전체검색] hits:\", len(hits))\n",
    "    return None, hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9eb934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[token 후보]: ['음주', '전후에', '타이레놀', '먹어도돼']\n",
      "[선택된 token]: 타이레놀 | 후보 수: 7\n",
      "1. 우먼스타이레놀정\n",
      "2. 타이레놀콜드-에스정\n",
      "3. 어린이타이레놀현탁액(아세트아미노펜)\n",
      "4. 타이레놀8시간이알서방정(아세트아미노펜)\n",
      "5. 타이레놀산500밀리그램(아세트아미노펜)\n",
      "6. 타이레놀정500밀리그람(아세트아미노펜)\n",
      "7. 어린이타이레놀산160밀리그램(아세트아미노펜)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번호 선택(Enter=1):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 drug_name: 타이레놀산500밀리그램(아세트아미노펜)\n",
      "[필터검색] hits: 6\n"
     ]
    }
   ],
   "source": [
    "_, hits = search_from_user_query(\"음주 전후에 타이레놀 먹어도돼?\", k=6, top_n=10, interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c7730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서 수: 6\n",
      "{'sub_count': 6, 'pro_type': '일반의약품', 'chunk_id': 'a93187414074-warn-1', 'drug_id': 'a93187414074', 'drug_name': '타이레놀산500밀리그램(아세트아미노펜)', 'drug_name_key': '타이레놀산500밀리그램(아세트아미노펜)', 'section': '주의사항', 'sub_index': 1, 'active_ingredient': '[M262287]아세트아미노펜 과립'}\n",
      "【약물명】타이레놀산500밀리그램(아세트아미노펜)\n",
      "【주성분】[M262287]아세트아미노펜 과립\n",
      "【구분】일반의약품\n",
      "【섹션】주의사항 (1/6)\n",
      "---\n",
      "1. 경고\n",
      "1) 매일 세잔 이상 정기적으로 술을 마시는 사람이 이 약이나 다른 해열 진통제를 복용해야 할 경우 반드시 의\n",
      "사 또는 약사와 상의해야 한다. 이러한 사람이 이 약을 복용하면 간손상이 유발될 수 있다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"검색된 문서 수: {len(hits)}\")\n",
    "print(hits[0].metadata)\n",
    "print(hits[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a24d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-12 04:43:21 [config.py:2614] Casting torch.float16 to torch.bfloat16.\n",
      "INFO 02-12 04:43:25 [config.py:585] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 02-12 04:43:25 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62390390c90b494787422c5360455b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/143 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-12 04:43:27 [utils.py:2181] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 02-12 04:43:30 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 02-12 04:43:31 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='eddyfox8812/llama-3-8b-otc-rag-ko-checkpotint-594', speculative_config=None, tokenizer='eddyfox8812/llama-3-8b-otc-rag-ko-checkpotint-594', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=eddyfox8812/llama-3-8b-otc-rag-ko-checkpotint-594, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 02-12 04:43:31 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c240f36a5d0>\n",
      "INFO 02-12 04:43:32 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 02-12 04:43:32 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 02-12 04:43:32 [gpu_model_runner.py:1174] Starting to load model eddyfox8812/llama-3-8b-otc-rag-ko-checkpotint-594...\n",
      "WARNING 02-12 04:43:32 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 02-12 04:43:36 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 02-12 04:43:44 [weight_utils.py:281] Time spent downloading weights for eddyfox8812/llama-3-8b-otc-rag-ko-checkpotint-594: 8.294269 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.72it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:03,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:02,  2.08s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.07s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 04:43:53 [loader.py:447] Loading weights took 8.30 seconds\n",
      "INFO 02-12 04:43:53 [gpu_model_runner.py:1186] Model loading took 14.9596 GB and 21.143287 seconds\n",
      "INFO 02-12 04:44:00 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/ecb5e4a8af/rank_0_0 for vLLM's torch.compile\n",
      "INFO 02-12 04:44:00 [backends.py:425] Dynamo bytecode transform time: 6.96 s\n",
      "INFO 02-12 04:44:02 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 02-12 04:44:26 [backends.py:144] Compiling a graph for general shape takes 25.28 s\n",
      "INFO 02-12 04:44:38 [monitor.py:33] torch.compile takes 32.24 s in total\n",
      "INFO 02-12 04:44:39 [kv_cache_utils.py:566] GPU KV cache size: 397,232 tokens\n",
      "INFO 02-12 04:44:39 [kv_cache_utils.py:569] Maximum concurrency for 8,192 tokens per request: 48.49x\n",
      "INFO 02-12 04:44:54 [gpu_model_runner.py:1534] Graph capturing finished in 15 secs, took 0.52 GiB\n",
      "INFO 02-12 04:44:54 [core.py:151] init engine (profile, create kv cache, warmup model) took 60.93 seconds\n"
     ]
    }
   ],
   "source": [
    "vllm_model = LLM(\n",
    "    model = \"eddyfox8812/llama-3-8b-otc-rag-ko-checkpotint-594\",\n",
    "    dtype = \"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de06676",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature = 0,\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 기본 chat template 확인 : {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"eddyfox8812/llama-3-8b-otc-rag-ko-checkpotint-594\")\n",
    "print(f\"모델의 기본 chat template 확인 : {tokenizer.chat_template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "apply_chat_template 예시 : \n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 도움이 되는 AI 어시스턴트 입니다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "안녕하세요, 도와주세요<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_messages = [\n",
    "    {\"role\" : \"system\", \"content\" : \"당신은 도움이 되는 AI 어시스턴트 입니다.\"},\n",
    "    {\"role\" : \"user\" , \"content\" : \"안녕하세요, 도와주세요\" }\n",
    "]\n",
    "formatted_example = tokenizer.apply_chat_template(\n",
    "    example_messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True\n",
    ")\n",
    "print(\"\\napply_chat_template 예시 : \")\n",
    "print(formatted_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"당신은 제공된 [문서]만 근거로 답하는 의약품 RAG 어시스턴트입니다.\n",
    "\n",
    "필수 규칙:\n",
    "- 답변에는 반드시 [문서]에서 인용한 근거를 포함해야 합니다.\n",
    "- 최소 1개 이상, 최대 2개의 '직접 인용'을 포함하세요(문서의 문장을 그대로).\n",
    "- 직접 인용은 아래 형식으로만 작성:\n",
    "  근거: [문서N] \"인용문\"\n",
    "- 문서에 근거가 없으면 그때만 \"문서에 근거가 부족합니다\"라고 말하세요.\n",
    "\n",
    "출력 형식(반드시 지킬 것):\n",
    "결론: (피해야 함/주의 필요/가능)\n",
    "이유: (문서 근거 요약 2~3문장)\n",
    "근거: [문서N] \"...\"\n",
    "(필요하면 근거 2개까지)\n",
    "\n",
    "[문서]\n",
    "{search_result}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32842923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs, max_chars_per_doc=1200, include_meta=True):\n",
    "    if not docs:\n",
    "        return \"관련 문서를 찾지 못했습니다.\"\n",
    "\n",
    "    formatted_results = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        text = (doc.page_content or \"\").strip()\n",
    "        if len(text) > max_chars_per_doc:\n",
    "            text = text[:max_chars_per_doc] + \" ...[truncated]\"\n",
    "\n",
    "        if include_meta:\n",
    "            m = doc.metadata or {}\n",
    "            header = f\"문서{i} | drug_name={m.get('drug_name')} | section={m.get('section')} | chunk_id={m.get('chunk_id')}\"\n",
    "            formatted_results.append(f\"{header}\\n{text}\")\n",
    "        else:\n",
    "            formatted_results.append(f\"문서{i} : {text}\")\n",
    "\n",
    "    return \"\\n----\\n\".join(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e725267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_drugs(token: str, top_n=10):\n",
    "    token = token.strip()\n",
    "    cands = [d for d in drug_names if token in d]\n",
    "    cands = sorted(cands, key=len)\n",
    "    return cands[:top_n]\n",
    "\n",
    "def choose_best_token(user_query: str, top_n=10):\n",
    "    tokens = re.findall(r\"[가-힣A-Za-z0-9]+\", user_query)\n",
    "    tokens = [t for t in tokens if len(t) >= 2]\n",
    "\n",
    "    best_token, best_cands = None, []\n",
    "    for t in tokens:\n",
    "        cands = candidate_drugs(t, top_n=top_n)\n",
    "        if len(cands) > len(best_cands):\n",
    "            best_token, best_cands = t, cands\n",
    "\n",
    "    return best_token, best_cands, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab514e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_response(question, retriever, tokenizer, vllm_model, sampling_params) : \n",
    "    \"\"\"\n",
    "    검색 결과와 질문을 기반으로 RAG 응답을 생성하는 함수\n",
    "    \"\"\"\n",
    "\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    formatted_results = format_docs(retrieved_docs)\n",
    "\n",
    "    formatted_system = system_message.format(search_result = formatted_results)\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : formatted_system},\n",
    "        {\"role\" : \"user\", \"content\" : question}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt = True\n",
    "    )\n",
    "\n",
    "    outputs = vllm_model.generate([prompt],sampling_params)\n",
    "\n",
    "    response = outputs[0].outputs[0].text\n",
    "\n",
    "    return response, formatted_results, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2355d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문:  술마시고 타이레놀 먹어도 돼?\n"
     ]
    }
   ],
   "source": [
    "question = \"술마시고 타이레놀 먹어도 돼?\"\n",
    "print(\"질문: \", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91e6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[token 후보]: ['술마시고', '타이레놀', '먹어도']\n",
      "[선택된 token]: 타이레놀 | 후보 수: 7\n",
      "\n",
      "[drug_name 후보 목록]\n",
      "1. 우먼스타이레놀정\n",
      "2. 타이레놀콜드-에스정\n",
      "3. 어린이타이레놀현탁액(아세트아미노펜)\n",
      "4. 타이레놀8시간이알서방정(아세트아미노펜)\n",
      "5. 타이레놀산500밀리그램(아세트아미노펜)\n",
      "6. 타이레놀정500밀리그람(아세트아미노펜)\n",
      "7. 어린이타이레놀산160밀리그램(아세트아미노펜)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번호 선택(Enter=1):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 drug_name: 타이레놀산500밀리그램(아세트아미노펜)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.86s/it, est. speed input: 582.42 toks/s, output: 78.61 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서 수: 3\n",
      "최종답변:\n",
      " 결론: 술을 마시고 타이레놀(아세트아미노펜)을 복용하는 것은 주의가 필요합니다.\n",
      "\n",
      "이유: 매일 세 잔 이상의 술을 마시는 사람이 이 약이나 다른 해열 진통제를 복용해야 할 경우, 반드시 의사 또는 약사와 상의해야 합니다. 이러한 경우에는 간손상이 유발될 수 있습니다. 따라서 술을 마시고 타이레놀을 복용하는 경우에도 이러한 경고 사항을 염두에 두고 복용해야 합니다. [문서1] \"매일 세잔 이상 정기적으로 술을 마시는 사람이 이 약이나 다른 해열 진통제를 복용해야 할 경우 반드시 의사 또는 약사와 상의해야 한다. 이러한 사람이 이 약을 복용하면 간손상이 유발될 수 있다.\" [문서2] \"이 약을 복용하지 말 것\"에 포함된 \"술을 마시는 사람\"에 대한 경고 사항을 참고할 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chosen_drug_name = pick_drug_name(question, top_n=10)\n",
    "\n",
    "if chosen_drug_name:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3, \"filter\": {\"drug_name\": chosen_drug_name}})\n",
    "else:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "response, formatted_results, retrieved_docs = generate_rag_response(\n",
    "    question, retriever, tokenizer, vllm_model, sampling_params\n",
    ")\n",
    "\n",
    "print(\"검색된 문서 수:\", len(retrieved_docs))\n",
    "print(\"최종답변:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf375ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[token 후보]: ['생리통에', '탁센', '먹어도']\n",
      "[선택된 token]: 탁센 | 후보 수: 7\n",
      "\n",
      "[drug_name 후보 목록]\n",
      "1. 탁센엠지연질캡슐\n",
      "2. 탁센이브연질캡슐\n",
      "3. 탁센레이디연질캡슐\n",
      "4. 탁센연질캡슐(나프록센)\n",
      "5. 탁센400이부프로펜연질캡슐\n",
      "6. 탁센덱시연질캡슐(덱시부프로펜)\n",
      "7. 탁센아세트아미노펜정500밀리그램(아세트아미노펜)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번호 선택(Enter=1):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 drug_name: 탁센400이부프로펜연질캡슐\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.41s/it, est. speed input: 1060.92 toks/s, output: 76.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서 수: 3\n",
      "최종답변:\n",
      " 생리통에 탁센400이부프로펜 연질캡슐을 복용해도 되는지에 대한 질문입니다. 이 약물은 일반적으로 생리통 치료에 사용되지 않으며, 복용 시 주의사항이 명시되어 있습니다. 위장관 궤양이나 출혈 환자, 심한 혈액 이상이나 간장애 환자, 심장 기능 부전 환자 등은 이 약을 복용하지 말아야 합니다. 또한, 고혈압 환자나 심혈관계 질환을 가진 환자는 신중히 고려해야 하며, 고용량 사용 시 위험이 증가할 수 있습니다. 따라서 생리통에 탁센을 복용하는 것은 권장되지 않으며, 대신 다른 대체 치료법을 고려하는 것이 좋습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"생리통에 탁센 먹어도 돼?\"\n",
    "\n",
    "chosen_drug_name = pick_drug_name(question, top_n=10) \n",
    "\n",
    "if chosen_drug_name:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3, \"filter\": {\"drug_name\": chosen_drug_name}})\n",
    "else:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "response, formatted_results, retrieved_docs = generate_rag_response(\n",
    "    question, retriever, tokenizer, vllm_model, sampling_params\n",
    ")\n",
    "\n",
    "print(\"검색된 문서 수:\", len(retrieved_docs))\n",
    "print(\"최종답변:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e35e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[token 후보]: ['임산부가', '코트리나', '먹어도']\n",
      "[선택된 token]: 코트리나 | 후보 수: 1\n",
      "\n",
      "[drug_name 후보 목록]\n",
      "1. 코트리나캡슐\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번호 선택(Enter=1):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 drug_name: 코트리나캡슐\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 1402.02 toks/s, output: 75.81 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서 수: 3\n",
      "최종답변:\n",
      " 임산부는 코트리나캡슐을 복용하지 말아야 합니다. 이 약물의 섹션 \"주의사항\"에 명시된 바와 같이, 임부 및 수유부는 이 약을 복용하지 말아야 한다고 되어 있습니다. 따라서 임산부는 코트리나캡슐을 복용하지 않는 것이 좋습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"임산부가 코트리나 먹어도 돼?\"\n",
    "\n",
    "chosen_drug_name = pick_drug_name(question, top_n=10)\n",
    "\n",
    "if chosen_drug_name:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3, \"filter\": {\"drug_name\": chosen_drug_name}})\n",
    "else:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "response, formatted_results, retrieved_docs = generate_rag_response(\n",
    "    question, retriever, tokenizer, vllm_model, sampling_params\n",
    ")\n",
    "\n",
    "print(\"검색된 문서 수:\", len(retrieved_docs))\n",
    "print(\"최종답변:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[token 후보]: ['유당불내증', '있는데', '쎄파렉신', '먹어도']\n",
      "[선택된 token]: 쎄파렉신 | 후보 수: 2\n",
      "\n",
      "[drug_name 후보 목록]\n",
      "1. 쎄파렉신캡슐(은교산)\n",
      "2. 쎄파렉신연조엑스(은교산)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번호 선택(Enter=1):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 drug_name: 쎄파렉신캡슐(은교산)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 3315.07 toks/s, output: 64.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 문서 수: 3\n",
      "최종답변:\n",
      " 문서에 유당불내증에 대한 정보는 제공되지 않았습니다. 따라서 이 질문에 대한 답변을 찾기 어렵습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"유당불내증 있는데 쎄파렉신 먹어도 돼?\"\n",
    "\n",
    "chosen_drug_name = pick_drug_name(question, top_n=10)\n",
    "\n",
    "if chosen_drug_name:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3, \"filter\": {\"drug_name\": chosen_drug_name}})\n",
    "else:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "response, formatted_results, retrieved_docs = generate_rag_response(\n",
    "    question, retriever, tokenizer, vllm_model, sampling_params\n",
    ")\n",
    "\n",
    "print(\"검색된 문서 수:\", len(retrieved_docs))\n",
    "print(\"최종답변:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc1bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1289/1812061586.py:72: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://473b3cb0a8f8d2be03.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://473b3cb0a8f8d2be03.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 874.98 toks/s, output: 80.39 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.69s/it, est. speed input: 624.13 toks/s, output: 78.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it, est. speed input: 1340.02 toks/s, output: 79.18 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it, est. speed input: 608.88 toks/s, output: 82.18 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it, est. speed input: 1078.21 toks/s, output: 76.02 toks/s]\n"
     ]
    }
   ],
   "source": [
    "def get_candidates(user_query, top_n=10):\n",
    "    token, cands, _ = choose_best_token(user_query, top_n=top_n)\n",
    "    return token, cands\n",
    "\n",
    "def chatbot_respond(message, history, pending):\n",
    "    if pending is not None:\n",
    "        sel = message.strip()\n",
    "\n",
    "        try:\n",
    "            sel_idx = 1 if sel == \"\" else int(sel)\n",
    "        except ValueError:\n",
    "            return \"번호(예: 1,2,3...)로 입력해줘. (Enter면 1번)\", pending\n",
    "\n",
    "        cands = pending[\"cands\"]\n",
    "        original_q = pending[\"question\"]\n",
    "\n",
    "        if not cands:\n",
    "            return \"후보가 비어있어. 다시 질문을 입력해줘.\", None\n",
    "\n",
    "        sel_idx = max(1, min(sel_idx, len(cands)))\n",
    "        chosen = cands[sel_idx - 1]\n",
    "\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": 3, \"filter\": {\"drug_name\": chosen}}\n",
    "        )\n",
    "\n",
    "        response, _, _ = generate_rag_response(\n",
    "            question=original_q,\n",
    "            retriever=retriever,\n",
    "            tokenizer=tokenizer,\n",
    "            vllm_model=vllm_model,\n",
    "            sampling_params=sampling_params,\n",
    "        )\n",
    "\n",
    "        return f\"선택됨: {chosen}\\n\\n{response}\", None\n",
    "\n",
    "    token, cands = get_candidates(message, top_n=10)\n",
    "\n",
    "    if not cands:\n",
    "\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "        response, _, _ = generate_rag_response(\n",
    "            question=message,\n",
    "            retriever=retriever,\n",
    "            tokenizer=tokenizer,\n",
    "            vllm_model=vllm_model,\n",
    "            sampling_params=sampling_params,\n",
    "        )\n",
    "        return response, None\n",
    "\n",
    "    menu = \"\\n\".join([f\"{i}. {name}\" for i, name in enumerate(cands, 1)])\n",
    "    안내 = (\n",
    "        \"약물명이 여러 개로 검색돼.\\n\"\n",
    "        \"아래 목록에서 **번호**를 입력해 선택해줘. (Enter면 1번)\\n\\n\"\n",
    "        f\"[token={token} | 후보 {len(cands)}개]\\n\"\n",
    "        f\"{menu}\"\n",
    "    )\n",
    "\n",
    "    return 안내, {\"question\": message, \"cands\": cands}\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    pending = gr.State(None)\n",
    "\n",
    "    gr.ChatInterface(\n",
    "        fn=chatbot_respond,\n",
    "        additional_inputs=[pending],\n",
    "        additional_outputs=[pending],\n",
    "        title=\"일반의약품 복용방법 및 주의사항 RAG챗봇\",\n",
    "        description=\"질문을 입력하면 후보 약물명이 나오고, 번호 입력으로 선택할 수 있습니다.\",\n",
    "        examples=[\n",
    "    [\"술마시고 타이레놀 먹어도 돼?\", None],\n",
    "    [\"덱시부루펜은 임산부가 먹어도 돼?\", None],\n",
    "    [\"7살 아이가 코트리나 먹어도 되는지 알려줘\", None],\n",
    "],\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
